<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Deep Learning Specialization | Course 1 - Neural Networks and Deep Learning | pandalandala's</title><meta name="keywords" content="DeepLearning"><meta name="author" content="zxr"><meta name="copyright" content="zxr"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Deep Learning Specialization Course 1&#x2F;5 by Deeplearning.AI on Coursera">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Specialization | Course 1 - Neural Networks and Deep Learning">
<meta property="og:url" content="https://pandalandala.github.io/2021/07/11/Deep-Learning-Specialization-Course-1-Neural-Networks-and-Deep-Learning/index.html">
<meta property="og:site_name" content="pandalandala&#39;s">
<meta property="og:description" content="Deep Learning Specialization Course 1&#x2F;5 by Deeplearning.AI on Coursera">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206052009945.png">
<meta property="article:published_time" content="2021-07-11T12:19:04.000Z">
<meta property="article:modified_time" content="2022-06-05T17:18:06.265Z">
<meta property="article:author" content="zxr">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206052009945.png"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419174.png"><link rel="canonical" href="https://pandalandala.github.io/2021/07/11/Deep-Learning-Specialization-Course-1-Neural-Networks-and-Deep-Learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Deep Learning Specialization | Course 1 - Neural Networks and Deep Learning',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-06 01:18:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/customizeavatar.css"> <link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 6.1.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419173.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206052009945.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">pandalandala's</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Deep Learning Specialization | Course 1 - Neural Networks and Deep Learning</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2021-07-11T12:19:04.000Z" title="发表于 2021-07-11 20:19:04">2021-07-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/DeepLearning/">DeepLearning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>33分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Deep Learning Specialization | Course 1 - Neural Networks and Deep Learning"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="C1-1-深度学习概论-Introduction-to-Deep-Learning"><a href="#C1-1-深度学习概论-Introduction-to-Deep-Learning" class="headerlink" title="C1-1 深度学习概论(Introduction to Deep Learning)"></a>C1-1 深度学习概论(Introduction to Deep Learning)</h1><h2 id="1-1-欢迎来到深度学习工程师微专业-Welcome"><a href="#1-1-欢迎来到深度学习工程师微专业-Welcome" class="headerlink" title="1.1 欢迎来到深度学习工程师微专业(Welcome)"></a>1.1 欢迎来到深度学习工程师微专业(Welcome)</h2><h2 id="1-2-什么是神经网络-What-is-a-Neural-Network"><a href="#1-2-什么是神经网络-What-is-a-Neural-Network" class="headerlink" title="1.2 什么是神经网络?(What is a Neural Network)"></a>1.2 什么是神经网络?(What is a Neural Network)</h2><p>这是一种强大的学习算法，灵感来自大脑的工作方式。</p>
<ul>
<li><p>例1：单神经网络给定房地产市场上房屋大小的数据，你想要拟合一个函数来预测它们的价格。这是一个线性回归问题，因为价格作为规模的函数是一个连续的产出。我们知道价格不可能是负的，所以我们创建了一个从0开始的<strong>“修正线性单元”函数（ReLU function）</strong>。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052048410.png" alt="image-20220605204809353" style="zoom:80%;">

<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052100123.png" alt="image-20220605210018069" style="zoom:80%;">

<p>输入为房子的大小x，输出为价格y，“神经元”实现了ReLU函数（蓝线）。</p>
</li>
<li><p>例2：多重神经网络。房子的价格还会受到其他因素的影响，比如面积、卧室数量、邮编和财富。神经网络的作用是预测价格，并自动生成隐藏的单位。我们只需要给出输入x和输出y来进行监督学习。值得注意的是神经网络给予了足够多的关于x和y的数据，给予了足够的训练样本。神经网络非常擅长计算从x到y的精准映射函数。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052100338.png" alt="image-20220605210049275" style="zoom:80%;"></li>
</ul>
<h2 id="1-3-神经网络的监督学习-Supervised-Learning-with-Neural-Networks"><a href="#1-3-神经网络的监督学习-Supervised-Learning-with-Neural-Networks" class="headerlink" title="1.3 神经网络的监督学习(Supervised Learning with Neural Networks)"></a>1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</h2><p>也许对于房地产和在线广告来说可能是相对的标准一些的神经网络，正如我们之前见到的。对于图像应用，我们经常在神经网络上使用<strong>卷积（Convolutional Neural Network）</strong>，通常缩写为<strong>CNN</strong>。对于序列数据，例如音频，有一个时间组件，随着时间的推移，音频被播放出来，作为<strong>一维时间序列（one-dimensional time series / temporal sequence）</strong>，所以音频是最自然的表现。对于序列数据，经常使用<strong>RNN</strong>，一种<strong>递归神经网络（Recurrent Neural Network</strong>），语言、英语和汉语字母表或单词都是逐个出现的，所以语言也是最自然的序列数据，因此更复杂的RNN版本经常用于这些应用。</p>
<p>对于更复杂的应用比如自动驾驶，给出一张图片，可能会显示更多的CNN卷积神经网络结构，其中的雷达信息是完全不同的，可能会有一些更复杂的混合的神经网络结构。</p>
<p>从历史经验上看，处理非结构化数据是很难的，与结构化数据比较，让计算机理解非结构化数据很难，而人类进化得非常善于理解音频信号和图像，文本是一个更近代的发明，但是人们真的很擅长解读非结构化数据。</p>
<p>多亏深度学习和神经网络，计算机现在能更好地解释非结构化数据，这为我们创造了机会。许多新的应用被使用，例如语音识别、图像识别、自然语言文字处理。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052101334.png" alt="image-20220605210112265" style="zoom: 67%;">

<h2 id="1-4-为什么深度学习会兴起？-Why-is-Deep-Learning-taking-off"><a href="#1-4-为什么深度学习会兴起？-Why-is-Deep-Learning-taking-off" class="headerlink" title="1.4 为什么深度学习会兴起？(Why is Deep Learning taking off?)"></a>1.4 为什么深度学习会兴起？(Why is Deep Learning taking off?)</h2><p>由于社会的数字化、更快的计算速度和神经网络算法发展中的创新，深度学习正在兴起。要使神经网络达到高水平的表现，必须考虑两件事:能够训练一个足够大的神经网络，以及大量的标签数据。神经网络的训练过程是迭代的。事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是<strong>要么训练一个更大的神经网络，要么投入更多的数据</strong>。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052101164.png" alt="image-20220605210139105" style="zoom:80%;">

<p>训练神经网络可能需要很长时间，这会影响你的工作效率。更快的计算速度有助于迭代和改进新算法。通过将<strong>Sigmod</strong>函数转换成ReLU函数，便能够使得一个叫做梯度下降（gradient descent）的算法运行的更快，这就是一个相对比较简单的算法创新的例子。</p>
<h2 id="1-5-关于这门课-About-this-Course"><a href="#1-5-关于这门课-About-this-Course" class="headerlink" title="1.5 关于这门课(About this Course)"></a>1.5 关于这门课(About this Course)</h2><h2 id="1-6-课程资源-Course-Resources"><a href="#1-6-课程资源-Course-Resources" class="headerlink" title="1.6 课程资源(Course Resources)"></a>1.6 课程资源(Course Resources)</h2><p>Contact us: <a href="mailto:feedback@deeplearning.ai">feedback@deeplearning.ai</a></p>
<p>Companies: <a href="mailto:enterprise@deeplearning.ai">enterprise@deeplearning.ai</a></p>
<p>Universities: <a href="mailto:academic@deeplearning.ai">academic@deeplearning.ai</a></p>
<h1 id="C1-2-神经网络基础-Basics-of-Neural-Network-programming"><a href="#C1-2-神经网络基础-Basics-of-Neural-Network-programming" class="headerlink" title="C1-2 神经网络基础(Basics of Neural Network programming)"></a>C1-2 神经网络基础(Basics of Neural Network programming)</h1><h2 id="2-1-二分类-Binary-Classification"><a href="#2-1-二分类-Binary-Classification" class="headerlink" title="2.1 二分类(Binary Classification)"></a>2.1 二分类(Binary Classification)</h2><p>在二元分类问题中，结果是一个离散值的输出。例如：Cat vs Non-Cat训练的目标是训练一个分类器，使其输入是一幅由特征向量x表示的图像，并预测对应的标签y是1(Cat)还是0(Non-Cat)。图像以三个独立的矩阵存储在计算机中，分别对应于图像的红、绿、蓝颜色通道。这三个矩阵的大小与图像相同，例如，猫图像的分辨率是64 × 64像素，三个矩阵(RGB)都是64 × 64。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052101873.png" alt="image-20220605210156806" style="zoom:80%;">

<p>$x$：表示一个维数据，为输入数据，维度为$(n_x,1)$； </p>
<p>$y$：表示输出结果，取值为$(0,1)$；</p>
<p>$(x^{(i)},y^{(i)})$：表示第$i$组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据；</p>
<p>$X=[x^{(1)},x^{(2)},…,x^{(m)}]$：表示所有的训练数据集的输入值，放在一个$n_x*m$的矩阵中，其中$m$表示样本数目; </p>
<p>$Y=[y^{(1)},y^{(2)},…,y^{(m)}]$：对应表示所有训练数据集的输出值，维度为$1*m$。</p>
<p>用一对$(x,y)$来表示一个单独的样本，$x$代表$n_x$维的特征向量，$y$表示标签(输出结果)只能为0或1。 而训练集由$m$个训练样本组成。有时候为了强调这是训练样本的个数，会写作$M_{train}$，当涉及到测试集的时候，我们会使用$M_{test}$来表示测试集的样本数。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052102423.png" alt="image-20220605210210365" style="zoom: 67%;">

<h2 id="2-2-逻辑回归-Logistic-Regression"><a href="#2-2-逻辑回归-Logistic-Regression" class="headerlink" title="2.2 逻辑回归(Logistic Regression)"></a>2.2 逻辑回归(Logistic Regression)</h2><p>Logistic回归是一种用于监督学习问题的学习算法，输出$y$全部为0或1。<strong>logistic回归的目标是最小化其预测和训练数据之间的误差。</strong></p>
<p>对于二元分类问题来讲，给定一个输入特征向量$X\in \mathbb{R} ^{n_x}$，$X$对应一张图片，你想识别这张图片看它是否是一只猫的图片，这时一个算法能够输出预测$\widehat{y}$，也就是对实际值$y$的估计。我们用$w$ (weights)来表示逻辑回归的参数，这也是一个$n_x$维向量（因为实际上是特征权重，维度与特征向量相同），参数里面还有$b$ (threshold)，这是一个实数（表示偏差）。所以给出输入$x$以及参数$w$和$b$之后，我们要产生输出预测值$\widehat{y}$。首先尝试$\widehat{y}=w^Tx+b$。这是一个线性函数，不适合用在二元分类问题上。因为想让$\widehat{y}$表示实际值$y$等于1的机率的话，$\widehat{y}$应该在0到1之间。因此在逻辑回归中，我们的输出应该是由上面得到的线性函数式子作为自变量的sigmoid函数中，公式为$\widehat{y}=\sigma(w^Tx+b)$，将线性函数转换为非线性函数。<strong>sigmoid函数</strong>的公式为$\sigma \left( z\right) =\dfrac{1}{1+e^{-z}}$，图像如下。因此当实现逻辑回归时，你的工作就是去让机器学习参数$w$以及$b$这样才使得$\widehat{y}$成为对$y=1$这一情况的概率的一个很好的估计。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052102522.png" alt="image-20220605210231452" style="zoom:80%;">

<h2 id="2-3-逻辑回归的损失函数（Logistic-Regression-Cost-Function）"><a href="#2-3-逻辑回归的损失函数（Logistic-Regression-Cost-Function）" class="headerlink" title="2.3 逻辑回归的损失函数（Logistic Regression Cost Function）"></a>2.3 逻辑回归的损失函数（Logistic Regression Cost Function）</h2><p>我们通过**损失函数$L(\widehat{y},y)$**衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数，其函数表达式和原理如下图所示。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814211359158.png" alt="image-20210814211359158" style="zoom:80%;">

<p>损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个<strong>算法的损失函数</strong>，算法的损失函数是对$m$个样本的损失函数求和然后除以$m$，如下图所示。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814211652210.png" alt="image-20210814211652210" style="zoom:80%;">

<p>损失函数只适用于像这样的单个训练样本，而代价函数是参数的总损失，所以在训练逻辑回归模型时候，我们需要找到合适的 $w$ 和$b$，来让代价函数$J$的总损失降到最低。</p>
<h2 id="2-4-梯度下降（Gradient-Descent）"><a href="#2-4-梯度下降（Gradient-Descent）" class="headerlink" title="2.4 梯度下降（Gradient Descent）"></a>2.4 梯度下降（Gradient Descent）</h2><ul>
<li><p><strong>初始化</strong>：在梯度下降法中，可以用如图那个小红点来初始化参数$w$和$b$，也可以采用随机初始化的方法，对于逻辑回归几乎所有的初始化方法都有效，因为函数是凸函数，无论在哪里初始化，应该达到同一点或大致相同的点。</p>
</li>
<li><p><strong>朝最陡的下坡方向走一步，不断地迭代</strong></p>
</li>
<li><p><strong>直到走到全局最优解或者接近全局最优解的地方</strong></p>
<p>下面将详细说明如何迭代。$:=$表示更新参数。 $a$表示学习率（learning rate），用来控制步长（step），即向下走一步的长度$\dfrac{dJ\left( w\right) }{dw}$就是函数$J(w)$对$w$求导，在代码中我们会使用$dw$表示这个结果。整个梯度下降法的迭代过程就是不断地向下走，直至逼近最小值点。</p>
</li>
</ul>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052102908.png" alt="image-20220605210253846" style="zoom:80%;">

<p>另外，每次迭代更新的修正表达式如下。</p>
<p>​                                                <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814213136470.png" alt="image-20210814213136470" style="zoom:67%;">              <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814213148117.png" alt="image-20210814213148117" style="zoom:67%;"></p>
<h2 id="2-5-导数（Derivatives）（略）"><a href="#2-5-导数（Derivatives）（略）" class="headerlink" title="2.5 导数（Derivatives）（略）"></a>2.5 导数（Derivatives）（略）</h2><h2 id="2-6-更多的导数例子（More-Derivative-Examples）（略）"><a href="#2-6-更多的导数例子（More-Derivative-Examples）（略）" class="headerlink" title="2.6 更多的导数例子（More Derivative Examples）（略）"></a>2.6 更多的导数例子（More Derivative Examples）（略）</h2><h2 id="2-7-计算图（Computation-Graph）"><a href="#2-7-计算图（Computation-Graph）" class="headerlink" title="2.7 计算图（Computation Graph）"></a>2.7 计算图（Computation Graph）</h2><h2 id="2-8-计算图导数（Derivatives-with-a-Computation-Graph）"><a href="#2-8-计算图导数（Derivatives-with-a-Computation-Graph）" class="headerlink" title="2.8 计算图导数（Derivatives with a Computation Graph）"></a>2.8 计算图导数（Derivatives with a Computation Graph）</h2><p>仅给出如下的计算图及其导数。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052103551.png" alt="image-20220605210310493" style="zoom: 80%;">

<h2 id="2-9-逻辑回归的梯度下降（Logistic-Regression-Gradient-Descent）"><a href="#2-9-逻辑回归的梯度下降（Logistic-Regression-Gradient-Descent）" class="headerlink" title="2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）"></a>2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814224159219.png" alt="image-20210814224159219" style="zoom:80%;">

<h2 id="2-10-m个样本的梯度下降-Gradient-Descent-on-m-Examples"><a href="#2-10-m个样本的梯度下降-Gradient-Descent-on-m-Examples" class="headerlink" title="2.10 m个样本的梯度下降(Gradient Descent on m Examples)"></a>2.10 m个样本的梯度下降(Gradient Descent on m Examples)</h2><p>对$m$个样本来说，其损失函数表达式如下：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814224823005.png" alt="image-20210814224823005" style="zoom:80%;">

<p>损失函数关于$w$和$b$的偏导数可以写成所有样本点偏导数和的平均形式：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210814224856685.png" alt="image-20210814224856685" style="zoom:80%;">

<p>代码实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">J=<span class="number">0</span>;dw1=<span class="number">0</span>;dw2=<span class="number">0</span>;db=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(<span class="number">1</span>-y(i)）log(<span class="number">1</span>-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J/= m;</span><br><span class="line">dw1/= m;</span><br><span class="line">dw2/= m;</span><br><span class="line">db/= m;</span><br><span class="line">w=w-alpha*dw</span><br><span class="line">b=b-alpha*db</span><br></pre></td></tr></tbody></table></figure>

<h2 id="2-11-向量化-Vectorization"><a href="#2-11-向量化-Vectorization" class="headerlink" title="2.11 向量化(Vectorization)"></a>2.11 向量化(Vectorization)</h2><p>向量化计算$w^Tx$将会非常快。以下是代码实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入numpy库</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]) <span class="comment">#创建一个数据a</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># [1 2 3 4]</span></span><br><span class="line"><span class="keyword">import</span> time <span class="comment">#导入时间库</span></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>) <span class="comment">#通过round随机得到两个一百万维度的数组</span></span><br><span class="line">tic = time.time() <span class="comment">#现在测量一下当前时间</span></span><br><span class="line"><span class="comment">#向量化的版本</span></span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Vectorized version:"</span> + <span class="built_in">str</span>(<span class="number">1000</span>*(toc-tic)) +<span class="string">"ms"</span>) <span class="comment">#打印一下向量化的版本的时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#继续增加非向量化的版本</span></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"For loop:"</span> + <span class="built_in">str</span>(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)<span class="comment">#打印for循环的版本的时间</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="2-12-向量化的更多例子（More-Examples-of-Vectorization）"><a href="#2-12-向量化的更多例子（More-Examples-of-Vectorization）" class="headerlink" title="2.12 向量化的更多例子（More Examples of Vectorization）"></a>2.12 向量化的更多例子（More Examples of Vectorization）</h2><p>我们将对m个样本的梯度下降进行向量化。向量化前的代码参照2.10，向量化后的代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">J=<span class="number">0</span>;dw=np.zeros((n_x,<span class="number">1</span>));db=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(<span class="number">1</span>-y(i)）log(<span class="number">1</span>-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw += x(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J/= m;</span><br><span class="line">dw/= m;</span><br><span class="line">db/= m;</span><br><span class="line">w=w-alpha*dw</span><br><span class="line">b=b-alpha*db</span><br></pre></td></tr></tbody></table></figure>

<h2 id="2-13-向量化logistic回归-Vectorizing-Logistic-Regression"><a href="#2-13-向量化logistic回归-Vectorizing-Logistic-Regression" class="headerlink" title="2.13 向量化logistic回归(Vectorizing Logistic Regression)"></a>2.13 向量化logistic回归(Vectorizing Logistic Regression)</h2><p>在向量化前，对每一个样本我们都要进行如下logistic回归的前向传播步骤。<br>$$<br>z^{(1)}=w^Tx^{(1)}+b\<br>z^{(2)}=w^Tx^{(2)}+b\<br>z^{(3)}=w^Tx^{(3)}+b<br>$$</p>
<p>$$<br>a^{(1)}=\sigma (z^{(1)})\<br>a^{(2)}=\sigma (z^{(2)})\<br>a^{(3)}=\sigma (z^{(3)})<br>$$</p>
<p>现在我们希望在一个步骤中计算$z_1$、$z_2$、$z_3$等。因此我们<strong>构建相应的矩阵</strong>并计算$Z$，即$(z^{\left( 1\right)}z^{\left( 2\right)}…z^{\left( m\right)})=w^Tx+(bb…b)$，我们将上式记作$Z=W^TX+(bb…b)$，其numpy命令为<code>Z = np.dot(w.T,X) + b</code>。接下来通过恰当地运用$\sigma$一次性计算所有$a$，即$A=(a^{\left( 1\right)}a^{\left( 2\right)}…a^{\left( m\right)})=\sigma(z)$。这就是在同一时间内完成一个所有$m$个训练样本的前向传播向量化计算方法。</p>
<p>在2.14中将利用向量化高效地计算反向传播并以此来计算梯度。</p>
<h2 id="2-14-向量化logistic回归的梯度计算（Vectorizing-Logistic-Regression’s-Gradient）"><a href="#2-14-向量化logistic回归的梯度计算（Vectorizing-Logistic-Regression’s-Gradient）" class="headerlink" title="2.14 向量化logistic回归的梯度计算（Vectorizing Logistic Regression’s Gradient）"></a>2.14 向量化logistic回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</h2><p>之前的梯度计算中我们一直利用$dz^{(i)}=a^{(i)}-y^{(i)}$，在向量化中，我们依旧<strong>构建相应的矩阵</strong>来计算$m$个训练数据的梯度：$dZ=A-Y=(a^{(1)}-y^{(1)}a^{(2)}-y^{(2)}…a^{(m)}-y^{(m)})$。所以我们仅需要一行代码，就能完成所有的计算。</p>
<p>至此，我们已经去掉了一个for循环。但我们仍有一个遍历训练集的循环用于计算$dw$和$db$，如下所示：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052108980.png" alt="image-20220605210851915" style="zoom:80%;">

<p>对这个for循环我们也使用向量化手段。对于$db$，不难想到在Python中的代码为<code>db=(1/m)*np.sum(dZ)</code>；对于$dw$，考虑其计算公式可得<code>dw=(1/m)*X*dz.T</code>，这样就避免了在训练集上使用for循环。</p>
<p>因此<strong>单次迭代</strong>的<strong>梯度下降算法</strong>的代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br><span class="line">dZ = A-Y</span><br><span class="line">dw = <span class="number">1</span>/m*np.dot(X,dZ.T)</span><br><span class="line">db = <span class="number">1</span>/m*np.<span class="built_in">sum</span>(dZ)</span><br><span class="line"></span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></tbody></table></figure>

<p>现在我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和求导，再利用后两个公式，梯度下降更新参数。我们的目的是不使用for循环，所以我们就通过一次迭代实现一次梯度下降，但如果你<u>希望多次迭代进行梯度下降，那么仍然需要for循环，放在最外层</u>。</p>
<h2 id="2-15-Python中的广播机制（Broadcasting-in-Python）"><a href="#2-15-Python中的广播机制（Broadcasting-in-Python）" class="headerlink" title="2.15 Python中的广播机制（Broadcasting in Python）"></a>2.15 Python中的广播机制（Broadcasting in Python）</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052108509.png" alt="image-20220605210807432" style="zoom:67%;">

<p>先来学习上面的代码。解释两条指令：解释一下<code>A.sum(axis = 0)</code>中的参数<code>axis</code>。<u>axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行</u>；而第二个<code>A/cal.reshape(1,4)</code>指令则调用了<strong>numpy中的广播机制</strong>。这里使用$3\times 4$的矩阵A除以$1\times 4$的矩阵$cal$。技术上来讲，其实并不需要再将矩阵 $cal$<code>reshape</code>(重塑)成$1\times 4$，因为矩阵$cal$本身已经是$1\times 4$了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作<code>reshape</code>是一个常量时间的操作，时间复杂度是$O(1)$，它的调用代价极低。</p>
<p><strong>numpy广播机制：如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为1的维度上进行</strong>，示例如下图。（注：后缘维度的轴长度：<code>A.shape[-1]</code> 即矩阵维度元组中的最后一个位置的值）</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052109930.png" alt="image-20220605210907856" style="zoom:80%;">

<h2 id="2-16-关于Python与numpy向量的使用（A-note-on-python-or-numpy-vectors）"><a href="#2-16-关于Python与numpy向量的使用（A-note-on-python-or-numpy-vectors）" class="headerlink" title="2.16 关于Python与numpy向量的使用（A note on python or numpy vectors）"></a>2.16 关于Python与numpy向量的使用（A note on python or numpy vectors）</h2><p>虽然在Python有广播的机制，但是在Python程序中，为了保证矩阵运算的正确性，可以使用reshape()函数来对矩阵设定所需要进行计算的维度，这是个好的习惯；</p>
<p>如果用<code>a = np.random.randn(5)</code>来定义一个向量，则这条语句生成的a的维度为 $(5, )$，既不是行向量也不是列向量，称为秩（rank）为1的array，如果对a进行转置，则会得到a本身，这在计算中会给我们带来一些问题。</p>
<p>如果需要定义$(5,1)$或者$(1,5)$向量，要使用下面标准的语句：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(5,1)</span><br><span class="line">b = np.random.randn(1,5)</span><br></pre></td></tr></tbody></table></figure>

<p>可以使用<strong>assert语句</strong>对向量或数组的维度进行判断。assert会对内嵌语句进行判断，即判断a的维度是不是$(5,1)$，如果不是，则程序在此处停止。使用assert语句也是一种很好的习惯，能够帮助我们及时检查、发现语句是否正确。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">assert(a.shape == (5,1))</span><br></pre></td></tr></tbody></table></figure>

<p>也可以使用<strong>reshape函数</strong>对数组设定所需的维度。</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.reshape((5,1))</span><br></pre></td></tr></tbody></table></figure>

<h2 id="2-17-Jupyter-x2F-iPython-Notebooks快速入门（Quick-tour-of-Jupyter-x2F-iPython-Notebooks）"><a href="#2-17-Jupyter-x2F-iPython-Notebooks快速入门（Quick-tour-of-Jupyter-x2F-iPython-Notebooks）" class="headerlink" title="2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）"></a>2.17 Jupyter/iPython Notebooks快速入门（Quick tour of Jupyter/iPython Notebooks）</h2><h2 id="2-18-logistic回归损失函数详解（Explanation-of-logistic-regression-cost-function）"><a href="#2-18-logistic回归损失函数详解（Explanation-of-logistic-regression-cost-function）" class="headerlink" title="2.18 logistic回归损失函数详解（Explanation of logistic regression cost function）"></a>2.18 logistic回归损失函数详解（Explanation of logistic regression cost function）</h2><p>在logistic回归中，我们约定如下式子：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052109027.png" alt="image-20220605210933954" style="zoom:67%;">

<p>需要指出的是我们讨论的是二分类问题的损失函数，因此$y$的取值只能是0或者1。上述的两个条件概率公式可以合并成如下公式：<br>$$<br>p(y|x)=\widehat{y}^y(1-\widehat{y})^{(1-y)}<br>$$<br>不难验证$(1)$式满足上述两个条件概率公式。因此$p(y|x)=\widehat{y}^y(1-\widehat{y})^{(1-y)}$就是$p(y|x)$的完整定义。由于对数函数是严格单调递增的函数，考虑最大化$log(p(y|x))$，即<br>$$<br>ylog\widehat{y}+(1-y)log(1-\widehat{y})<br>$$<br>而这就是前面提到的损失函数的负数$-L(\widehat{y},y)$。当训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），因此实现了在logistic回归中将损失函数最小化。</p>
<p>下面考虑<strong>在$m$个训练样本的整个训练集中如何表示logistic回归的损失函数</strong>。假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210817223810898.png" alt="image-20210817223810898" style="zoom:80%;">

<p>如果做最大似然估计，需要寻找一组参数，使得给定样本的观测值概率最大，但令这个概率最大化等价于令其对数最大化，在等式两边取对数：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052109320.png" alt="image-20220605210952259" style="zoom:80%;">

<p>由于训练模型时，目标是让成本函数最小化，所以我们不是直接用最大似然概率，要去掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外的常数因子$\dfrac{1}{m}$：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:\Users\zxr\AppData\Roaming\Typora\typora-user-images\image-20210817224031094.png" alt="image-20210817224031094" style="zoom:80%;">

<h1 id="C1-3-浅层神经网络-Shallow-neural-networks"><a href="#C1-3-浅层神经网络-Shallow-neural-networks" class="headerlink" title="C1-3 浅层神经网络(Shallow neural networks)"></a>C1-3 浅层神经网络(Shallow neural networks)</h1><h2 id="3-1-神经网络概述（Neural-Network-Overview）"><a href="#3-1-神经网络概述（Neural-Network-Overview）" class="headerlink" title="3.1 神经网络概述（Neural Network Overview）"></a>3.1 神经网络概述（Neural Network Overview）</h2><p>如下图所示是一个sigmoid单元以及其损失函数表达式。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052110946.png" alt="image-20220605211027881" style="zoom:80%;">

<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052111666.png" alt="image-20220605211111604" style="zoom:80%;">

<p><strong>把许多sigmoid单元堆叠起来可以形成一个神经网络</strong>，如下图所示。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052111778.png" alt="image-20220605211123710" style="zoom:80%;">

<p>它包含了之前计算的两个步骤：首先通过公式计算出值$z$，然后通过$\sigma(z)$计算值$a$。在这个神经网络第一层对应的3个节点，首先计算第一层网络中的各个节点相关的数$z^{[1]}$，接着计算$a^{[1]}$，再计算下一层网络；我们会使用符号$^{[m]}$表示第$m$层网络中节点相关的数，这些节点的集合被称为第$m$层网络。类似logistic回归，在计算后需要使用计算，接下来你需要使用另外一个线性方程对应的参数计算$z^{[2]}$，计算$a^{[2]}$，此时$a^{[2]}$就是整个神经网络最终的输出，也可用$\widehat{y}$表示神经网络的输出。整个计算过程如下：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052112599.png" alt="image-20220605211212532" style="zoom: 80%;">

<p>接下来看反向传播的神经网络计算过程，仅给出示意图：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052112975.png" alt="image-20220605211220914" style="zoom:80%;">

<h2 id="3-2-神经网络的表示（Neural-Network-Representation）"><a href="#3-2-神经网络的表示（Neural-Network-Representation）" class="headerlink" title="3.2 神经网络的表示（Neural Network Representation）"></a>3.2 神经网络的表示（Neural Network Representation）</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052112394.png" alt="image-20220605211237332" style="zoom:80%;">

<p>在如图所示的神经网络中，我们有输入特征$x_1$、$x_2$、$x_3$，它们被竖直地堆叠起来，这叫做神经网络的<strong>输入层</strong>；然后这里有另外一层我们称之为<strong>隐藏层</strong>（中间的的四个结点），这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值；在本例中最后一层只由一个结点构成，而这个只有一个结点的层被称为<strong>输出层</strong>，它负责产生预测值。上图所示的神经网络我们一般称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。</p>
<p>之前用向量$x$表示输入特征。这里有个可代替的记号$a^{[0]}$可以用来表示输入特征。$a$表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将$x$传递给隐藏层，所以我们将输入层的激活值称为$a^{[0]}$；下一层即隐藏层也同样会产生一些激活值，那么我将其记作$a^{[1]}$，所以具体地，这里的第一个单元或结点我们将其表示为$a_1^{[1]}$，第二个结点的值我们记为$a_2^{[1]}$，以此类推。所以这里的$a^{[1]}$是一个四维向量，如下图所示：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052112961.png" alt="image-20220605211251903" style="zoom:80%;">

<ul>
<li>由上面我们可以总结出，在神经网络中，我们以相邻两层为观测对象，前面一层作为输入，后面一层作为输出，两层之间的$w$参数矩阵大小为$(n_{out},n_{in})$，$b$参数矩阵大小为$(n_{out},1)$，这里是作为$z=wX+b$的线性关系来说明的，在神经网络中，$w^{[i]}=w^T$。</li>
<li>在logistic回归中，一般我们都会用$(n_{in},n_{out})$来表示参数大小，计算使用的公式为：$z=w^TX+b$。</li>
</ul>
<h2 id="3-3-计算一个神经网络的输出（Computing-a-Neural-Network’s-output）"><a href="#3-3-计算一个神经网络的输出（Computing-a-Neural-Network’s-output）" class="headerlink" title="3.3 计算一个神经网络的输出（Computing a Neural Network’s output）"></a>3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</h2><ul>
<li>用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先按步骤计算出$z$，然后以sigmoid函数为激活函数计算$z$（得出$a$），一个神经网络只是这样子做了好多次重复计算。</li>
</ul>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052113775.png" alt="image-20220605211304716" style="zoom:80%;">

<ul>
<li><p>接下来做向量化计算。</p>
<pre><code>               &lt;img src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052113612.png" alt="image-20220605211328531" style="zoom:80%;" /&gt;
</code></pre>
<p>接下来将了解如何能够计算出不止一个样本的神经网络输出，而是能一次性计算整个训练集的输出。</p>
</li>
</ul>
<h2 id="3-4-多样本向量化（Vectorizing-across-multiple-examples）"><a href="#3-4-多样本向量化（Vectorizing-across-multiple-examples）" class="headerlink" title="3.4 多样本向量化（Vectorizing across multiple examples）"></a>3.4 多样本向量化（Vectorizing across multiple examples）</h2><ul>
<li><p>如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让从1到实现这四个等式：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052113578.png" alt="image-20220605211343519" style="zoom:80%;">
</li>
<li><p>下面进行向量化：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052114103.png" alt="image-20220605211413033" style="zoom:80%;">

<p>因此：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052114371.png" alt="image-20220605211426304" style="zoom:80%;">

<p>对矩阵$A$、$Z$、$X$，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。下一节中将证明为什么这是一种正确向量化的实现。</p>
</li>
</ul>
<h2 id="3-5-向量化实现的解释（Justification-for-vectorized-implementation）"><a href="#3-5-向量化实现的解释（Justification-for-vectorized-implementation）" class="headerlink" title="3.5 向量化实现的解释（Justification for vectorized implementation）"></a>3.5 向量化实现的解释（Justification for vectorized implementation）</h2><p>对单个样本的计算要写成$z^{<a href="i">1</a>}=W^{[1]}x^{(i)}+b^{[1]}$这种形式，因为当有不同的训练样本时，将它们堆到矩阵$X$的各列中，那么它们的输出也就会相应的堆叠到矩阵$Z^{[1]}$的各列中。现在我们就可以直接计算矩阵$Z^{[1]}$加上$b^{[1]}$，因为列向量$b^{[1]}$和矩阵$Z^{[1]}$的列向量有着相同的尺寸，而Python的广播机制对于这种矩阵与向量直接相加的处理方式是，将向量与矩阵的每一列相加。 所以这一节只是说明了为什么公式$z^{<a href="i">1</a>}=W^{[1]}x^{(i)}+b^{[1]}$是前向传播的第一步计算的正确向量化实现，但事实证明，类似的分析可以发现，前向传播的其它步也可以使用非常相似的逻辑，即如果将输入按列向量横向堆叠进矩阵，那么通过公式计算之后，也能得到成列堆叠的输出。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052114889.png" alt="image-20220605211442805" style="zoom:80%;">

<h2 id="3-6-激活函数（Activation-functions）"><a href="#3-6-激活函数（Activation-functions）" class="headerlink" title="3.6 激活函数（Activation functions）"></a>3.6 激活函数（Activation functions）</h2><p>使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。到目前为止只用过sigmoid激活函数，但有时其他的激活函数效果会更好。更通常的情况下，使用不同的函数$g(z^{[1]})$，$g$可以是除了sigmoid函数以外的非线性函数。<strong>tanh函数</strong>或者双曲正切函数是总体上都优于sigmoid函数的激活函数。<br>$$<br>a=tanh(z)=\dfrac{e^z-e^{-z}}{e^z+e^{-z}}<br>$$<br>在机器学习另一个很流行的函数是：修正线性单元函数（ReLu函数）。这有一些选择激活函数的经验法则：如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择Relu函数。这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。有时，也会使用tanh激活函数，<u>但Relu的一个优点是：当是负值的时候，导数等于0。</u></p>
<p>这里也有另一个版本的ReLU被称为<strong>Leaky ReLU函数</strong>。当$z$是负值时，这个函数的值不是等于0，而是轻微的倾斜，如图。这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLU使用的并不多。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052115397.png" alt="image-20220605211500317" style="zoom:80%;">

<p><strong>ReLU函数和Leaky ReLU函数的优点是：</strong></p>
<ul>
<li><p>在$z$的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，在实践中，使用ReLU激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快。</p>
</li>
<li><p>sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而Relu和Leaky ReLU函数大于0部分都为常数，不会产生梯度弥散现象。（同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLU不会有此问题）</p>
</li>
</ul>
<p>$z$在ReLu函数的梯度一半都是0，但是，有足够的隐藏层使得$z$值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p>
<p>在选择自己神经网络的激活函数时，通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。</p>
<h2 id="3-7-为什么需要非线性激活函数？（why-need-a-nonlinear-activation-function-）"><a href="#3-7-为什么需要非线性激活函数？（why-need-a-nonlinear-activation-function-）" class="headerlink" title="3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）"></a>3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</h2><p>事实证明：要让你的神经网络能够计算出有趣的函数，你必须使用非线性激活函数。为了说明问题我们令$a^{[i]}=z^{[i]}$，那么这个模型的输出$y$仅仅只是输入特征$x$的线性组合。过程如下：</p>
<ul>
<li><p>$a^{[1]}=z^{[1]}=W^{[1]}x+b^{[1]}$</p>
</li>
<li><p>$a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}$</p>
</li>
<li><p>$a^{[2]}=z^{[2]}=W^{[2]}W^{[1]}x+W^{[2]}b^{[1]}+b^{[2]}$</p>
<p>$\Rightarrow   a^{[2]}=z^{[2]}=W^{‘}x+b^{‘}$</p>
</li>
</ul>
<p>事实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。在我们的简明案例中，事实证明如果你在隐藏层用线性激活函数，在输出层用sigmoid函数，那么这个模型的复杂度和没有任何隐藏层的标准logistic回归是一样的。</p>
<p>总而言之，<strong>绝大部分情况下不能在隐藏层用线性激活函数</strong>，可以用ReLU或者tanh或者leaky ReLU或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。</p>
<h2 id="3-8-激活函数的导数（Derivatives-of-activation-functions）"><a href="#3-8-激活函数的导数（Derivatives-of-activation-functions）" class="headerlink" title="3.8 激活函数的导数（Derivatives of activation functions）"></a>3.8 激活函数的导数（Derivatives of activation functions）</h2><p>导数是我们比较熟悉的，直接给出：</p>
<ul>
<li><p><strong>sigmoid</strong>：$g’(z)=g(z)(1-g(z))=a(1-a)$</p>
</li>
<li><p><strong>tanh</strong>：$g’(z)=1-(g’(z))^2=1-tanh^2(z)$</p>
</li>
<li><p><strong>ReLU</strong>：$g’\left( z\right) =\begin{cases}0(z&lt;0)\ 1(z&gt;0)\ undefined(z=0)\end{cases}$</p>
</li>
<li><p><strong>Leaky ReLU</strong>：$g’\left( z\right) =\begin{cases}0.01(z&lt;0)\ 1(z&gt;0)\ undefined(z=0)\end{cases}$</p>
</li>
</ul>
<h2 id="3-9-神经网络的梯度下降（Gradient-descent-for-neural-networks）"><a href="#3-9-神经网络的梯度下降（Gradient-descent-for-neural-networks）" class="headerlink" title="3.9 神经网络的梯度下降（Gradient descent for neural networks）"></a>3.9 神经网络的梯度下降（Gradient descent for neural networks）</h2><p>以本节中的浅层神经网络为例，我们给出神经网络的梯度下降法的公式。</p>
<ul>
<li>参数：$W^{[1]},b^{[1]},W^{[2]},b^{[2]}$；</li>
<li>输入层特征向量个数：$n_x=n^{[0]}$；</li>
<li>隐藏层神经元个数：$n^{[1]}$；</li>
<li>输出层神经元个数：$n^{[2]}=1$；</li>
<li>$W^{[1]}$的维度为$(n^{[1]},n^{[0]})$，$b^{[1]}$的维度为$(n^{[1]},1)$；</li>
<li>$W^{[2]}$的维度为$(n^{[2]},n^{[1]})$，$b^{[2]}$的维度为$(n^{[2]},1)$。</li>
</ul>
<p>下面为该例子的神经网络反向梯度下降公式（左）和其代码向量化（右）：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052118366.png" alt="image-20220605211835267" style="zoom: 67%;">

<h2 id="3-10（选修）直观理解反向传播（Backpropagation-intuition）"><a href="#3-10（选修）直观理解反向传播（Backpropagation-intuition）" class="headerlink" title="3.10（选修）直观理解反向传播（Backpropagation intuition）"></a>3.10（选修）直观理解反向传播（Backpropagation intuition）</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052118236.png" alt="image-20220605211809138" style="zoom: 67%;">

<h2 id="3-11-随机初始化（Random-Initialization）"><a href="#3-11-随机初始化（Random-Initialization）" class="headerlink" title="3.11 随机初始化（Random+Initialization）"></a>3.11 随机初始化（Random+Initialization）</h2><p>如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。</p>
<p>在初始化的时候，$W$参数要进行随机初始化，$b$则不存在对称性的问题它可以设置为0。</p>
<p>以2个输入，2个隐藏神经元为例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn((<span class="number">2</span>,<span class="number">2</span>))*<span class="number">0.01</span></span><br><span class="line">b = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure>

<p>这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则$Z=WX+b$所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p>
<h1 id="C1-4-深层神经网络-Deep-Neural-Networks"><a href="#C1-4-深层神经网络-Deep-Neural-Networks" class="headerlink" title="C1-4 深层神经网络(Deep Neural Networks)"></a>C1-4 深层神经网络(Deep Neural Networks)</h1><h2 id="4-1-深层神经网络（Deep-L-layer-neural-network）"><a href="#4-1-深层神经网络（Deep-L-layer-neural-network）" class="headerlink" title="4.1 深层神经网络（Deep L-layer neural network）"></a>4.1 深层神经网络（Deep L-layer neural network）</h2><p>有较多隐藏层的神经网络为<strong>深层神经网络</strong>。在不同层所拥有的神经元的数目，对于每层$l$都用$a^{[l]}$来记作$l$层激活后结果，我们会在后面看到在正向传播时，最终能你会计算出$a^{[l]}$。通过用激活函数$g$计算$a^{[l]}$，激活函数也被索引为层数$l$，然后我们用$w^{[l]}$来记作在$l$层计算$z^{[l]}$值的权重。类似的，$z^{[l]}$里的$b^{[l]}$也一样。</p>
<h2 id="4-2-深层网络中的前向传播（Forward-propagation-in-a-Deep-Network）"><a href="#4-2-深层网络中的前向传播（Forward-propagation-in-a-Deep-Network）" class="headerlink" title="4.2 深层网络中的前向传播（Forward propagation in a Deep Network）"></a>4.2 深层网络中的前向传播（Forward propagation in a Deep Network）</h2><p>前向传播可以归纳为多次迭代$z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$，$a^{[l]}=g^{[l]}(z^{[l]})$。向量化实现过程可以写成$Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$，$A^{[l]}=g^{[l]}(Z^{[l]})(A^{[0]}=X)$。这里只能用一个显式for循环，从1到$l$，然后一层接着一层去计算。</p>
<h2 id="4-3-核对矩阵的维数（Getting-your-matrix-dimensions-right）"><a href="#4-3-核对矩阵的维数（Getting-your-matrix-dimensions-right）" class="headerlink" title="4.3 核对矩阵的维数（Getting your matrix dimensions right）"></a>4.3 核对矩阵的维数（Getting your matrix dimensions right）</h2><p>对于第$l$层神经网络，单个样本其各个参数的矩阵维度为：</p>
<ul>
<li>$W^{[l]}:(n^{[l]},n^{[l-1]})$</li>
<li>$b^{[l]}:(n^{[l]},1)$</li>
<li>$dW^{[l]}:(n^{[l]},n^{[l-1]})$</li>
<li>$db^{[l]}:(n^{[l]},n^{[l-1]})$</li>
<li>$Z^{[l]}:(n^{[l]},1)$</li>
<li>$A^{[l]}:(n^{[l]},1)$</li>
</ul>
<h2 id="4-4-为什么使用深层表示？（Why-deep-representations-）"><a href="#4-4-为什么使用深层表示？（Why-deep-representations-）" class="headerlink" title="4.4 为什么使用深层表示？（Why deep representations?）"></a>4.4 为什么使用深层表示？（Why deep representations?）</h2><ul>
<li><strong>人脸识别和语音识别</strong></li>
</ul>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052118709.png" alt="image-20220605211859621" style="zoom:80%;">

<p>对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。</p>
<p>对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。</p>
<p>所以从上面的两个例子可以看出随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。</p>
<ul>
<li><strong>电路逻辑计算</strong></li>
</ul>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052119989.png" alt="image-20220605211913900" style="zoom:80%;">

<p>假定计算异或逻辑输出：$y=x_{1}\oplus x_{2}\oplus x_{3}\oplus… \oplus x_n $。</p>
<p>对于该运算，若果使用深度神经网络，每层将前一层的相邻的两单元进行异或，最后到一个输出，此时整个网络的层数为一个树形的形状，网络的深度为$O(log_2(n))$，共使用的神经元的个数为$n-1$。但是如果不适用深层网络，仅仅使用单隐层的网络（如右图所示），需要的神经元个数为$2^{n-1}$个 。同样的问题，但是深层网络要比浅层网络所需要的神经元个数要少得多。</p>
<h2 id="4-5-搭建深层神经网络块（Building-blocks-of-deep-neural-networks）"><a href="#4-5-搭建深层神经网络块（Building-blocks-of-deep-neural-networks）" class="headerlink" title="4.5 搭建深层神经网络块（Building blocks of deep neural networks）"></a>4.5 搭建深层神经网络块（Building blocks of deep neural networks）</h2><ul>
<li><p><strong>正向函数</strong>：在$l$层，你会有正向函数，输入$a^{[l-1]}$并且输出$a^{[l]}$，为了计算结果你需要用$W^{[l]}$和$b^{[l]}$，以及输出到缓存的$ z^{[l]}$。然后用作反向传播的反向函数，是另一个函数，输入$da^{[l]}$，输出$da^{[l-1]}$，你就会得到对激活函数的导数，也就是希望的导数值$da^{[l]}$。$a^{[l-1]}$是会变的，前一层算出的激活函数导数。在这个方块（第二个）里你需要$ W^{[l]}$和$ b^{[l]}$，最后你要算的是$dz^{[l]}$。然后这个方块（第三个）中，这个反向函数可以计算输出$dW^{[l]}$和$db^{[l]}$。</p>
</li>
<li><p><strong>反向函数</strong>：对反向传播的步骤而言，我们需要算一系列的反向迭代，就是这样反向计算梯度，你需要把$da^{[l]}$的值放在这里，然后这个方块会给我们$da^{[l-1]}$的值，以此类推，直到我们得到$da^{[2]}$和$da^{[l]}$，你还可以计算多一个输出值，就是$da^{[0]}$，但这其实是你的输入特征的导数，并不重要，起码对于训练监督学习的权重不算重要，你可以止步于此。反向传播步骤中也会输出$dW^{[l]}$和$db^{[l]}$，这会输出$dW^{[3]}$和$db^{[3]}$等等。目前为止你算好了所有需要的导数，稍微填一下这个流程图。</p>
<p>神经网络的一步训练从$a^{[0]}$开始，也就是然$x$后经过一系列正向传播计算得到$\widehat{y}$，之后再用输出值计算这个（第二行最后方块），再实现反向传播。现在你就有所有的导数项了也，$W$会在每一层被更新为$W=W-\alpha dW$，$b$也一样，$b=b-\alpha db$，反向传播就都计算完毕，我们有所有的导数值，那么这是神经网络一个梯度下降循环。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/posts/202206052119749.png" alt="image-20220605211932649" style="zoom:80%;"></li>
</ul>
<h2 id="4-6-前向传播和反向传播（Forward-and-backward-propagation）"><a href="#4-6-前向传播和反向传播（Forward-and-backward-propagation）" class="headerlink" title="4.6 前向传播和反向传播（Forward and backward propagation）"></a>4.6 前向传播和反向传播（Forward and backward propagation）</h2><ul>
<li><p><strong>前向传播（Forward propagation）</strong>：</p>
<p>Input：$a^{[l-1]}$</p>
<p>Output：$a^{[l]},cache(z^{[l]})$</p>
<ul>
<li><p>公式：<br>$$<br>z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\<br>a^{[l]}=g^{[l]}(z^{[l]})<br>$$</p>
</li>
<li><p>向量化程序：<br>$$<br>Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\<br>A^{[l]}=g^{[l]}(Z^{[l]})<br>$$</p>
</li>
</ul>
</li>
<li><p><strong>反向传播（Backward propagation）</strong>：</p>
<p>Input：$da^{[l]}$</p>
<p>Output：$da^{[l-1]},dW^{[l]},db^{[l]}$</p>
<ul>
<li><p>公式：<br>$$<br>dz^{[l]}=da^{[l]}*g’^{[l]}(z{[l]})\<br>dW^{[l]}=dz^{[l]}\cdot a^{[l-1]}\<br>db^{[l]}=dz^{[l]}\<br>da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}<br>$$<br>将$da^{[l-1]}$代入$dz^{[l]}$，有：<br>$$<br>dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}*g’^{[l]}(z{[l]})<br>$$</p>
</li>
<li><p>向量化程序：<br>$$<br>dZ^{[l]}=dA^{[l]}*g’^{[l]}(Z{[l]})\<br>dW^{[l]}=\dfrac{1}{m}dZ^{[l]}\cdot A^{[l-1]}\<br>db^{[l]}=\dfrac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)\<br>dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}<br>$$</p>
</li>
</ul>
</li>
</ul>
<h2 id="4-7-参数VS超参数（Parameters-vs-Hyperparameters）"><a href="#4-7-参数VS超参数（Parameters-vs-Hyperparameters）" class="headerlink" title="4.7 参数VS超参数（Parameters vs Hyperparameters）"></a>4.7 参数VS超参数（Parameters vs Hyperparameters）</h2><p>参数即是我们在过程中想要模型学习到的信息，如$W^{[l]}$，$b^{[l]}$。超参数即为控制参数的输出值的一些网络信息，也就是超参数的改变会导致最终得比如算法中的learning rate $a$（学习率）、iterations(梯度下降法循环的数量)、$L$（隐藏层数目）、$n^{[l]}$（隐藏层单元数目）、choice of activation function（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数$W$和$b$的值，所以它们被称作超参数。</p>
<p>实际上深度学习有很多不同的超参数，之后我们也会介绍一些其他的超参数，如momentum、mini batch size、regularization parameters等等。设置超参数有一条经验规律：经常试试不同的超参数，勤于检查结果，看看有没有更好的超参数取值，你将会得到设定超参数的直觉。</p>
<h2 id="4-8-深度学习和大脑的关联性（What-does-this-have-to-do-with-the-brain-）"><a href="#4-8-深度学习和大脑的关联性（What-does-this-have-to-do-with-the-brain-）" class="headerlink" title="4.8 深度学习和大脑的关联性（What does this have to do with the brain?）"></a>4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</h2><p>一个神经网络的逻辑单元可以看成是对一个生物神经元的过度简化，但迄今为止连神经科学家都很难解释究竟一个神经元能做什么，它可能是极其复杂的；它的一些功能可能真的类似logistic回归的运算，但单个神经元到底在做什么目前还没有人能够真正可以解释。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">zxr</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://pandalandala.github.io/2021/07/11/Deep-Learning-Specialization-Course-1-Neural-Networks-and-Deep-Learning/">https://pandalandala.github.io/2021/07/11/Deep-Learning-Specialization-Course-1-Neural-Networks-and-Deep-Learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://pandalandala.github.io" target="_blank">pandalandala's</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206052009945.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/05/03/Hello-world-0/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419169.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hello,world!</div></div></a></div><div class="next-post pull-right"><a href="/2021/07/20/About-TOPIK/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419172.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">About TOPIK</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/07/24/Deep-Learning-Specialization-Course-2-Improving-Deep-Neural-Networks-Hyperparameter-Tuning-Regularization-and-Optimization/" title="Deep Learning Specialization | Course 2 - Improving Deep Neural Networks - Hyperparameter Tuning, egularization and Optimization"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206052021019.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-07-24</div><div class="title">Deep Learning Specialization | Course 2 - Improving Deep Neural Networks - Hyperparameter Tuning, egularization and Optimization</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419173.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="avatar"/></div><div class="author-info__name">zxr</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/pandalandala" target="_blank" title="Github"><i class="fa-brands fa-github"></i></a><a class="social-icon" href="mailto:zxrshawn@icloud.com" target="_blank" title="Email"><i class="fa-solid fa-envelope"></i></a><a class="social-icon" href="https://www.worldcubeassociation.org/persons/2014ZHEN02" target="_blank" title="World Cube Association"><i class="fa-solid fa-cube"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#C1-1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA-Introduction-to-Deep-Learning"><span class="toc-text">C1-1 深度学习概论(Introduction to Deep Learning)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%BE%AE%E4%B8%93%E4%B8%9A-Welcome"><span class="toc-text">1.1 欢迎来到深度学习工程师微专业(Welcome)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E4%BB%80%E4%B9%88%E6%98%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-What-is-a-Neural-Network"><span class="toc-text">1.2 什么是神经网络?(What is a Neural Network)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Supervised-Learning-with-Neural-Networks"><span class="toc-text">1.3 神经网络的监督学习(Supervised Learning with Neural Networks)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%9A%E5%85%B4%E8%B5%B7%EF%BC%9F-Why-is-Deep-Learning-taking-off"><span class="toc-text">1.4 为什么深度学习会兴起？(Why is Deep Learning taking off?)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-%E5%85%B3%E4%BA%8E%E8%BF%99%E9%97%A8%E8%AF%BE-About-this-Course"><span class="toc-text">1.5 关于这门课(About this Course)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-6-%E8%AF%BE%E7%A8%8B%E8%B5%84%E6%BA%90-Course-Resources"><span class="toc-text">1.6 课程资源(Course Resources)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#C1-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80-Basics-of-Neural-Network-programming"><span class="toc-text">C1-2 神经网络基础(Basics of Neural Network programming)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E4%BA%8C%E5%88%86%E7%B1%BB-Binary-Classification"><span class="toc-text">2.1 二分类(Binary Classification)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-Logistic-Regression"><span class="toc-text">2.2 逻辑回归(Logistic Regression)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Logistic-Regression-Cost-Function%EF%BC%89"><span class="toc-text">2.3 逻辑回归的损失函数（Logistic Regression Cost Function）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent%EF%BC%89"><span class="toc-text">2.4 梯度下降（Gradient Descent）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E5%AF%BC%E6%95%B0%EF%BC%88Derivatives%EF%BC%89%EF%BC%88%E7%95%A5%EF%BC%89"><span class="toc-text">2.5 导数（Derivatives）（略）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%AF%BC%E6%95%B0%E4%BE%8B%E5%AD%90%EF%BC%88More-Derivative-Examples%EF%BC%89%EF%BC%88%E7%95%A5%EF%BC%89"><span class="toc-text">2.6 更多的导数例子（More Derivative Examples）（略）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7-%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%88Computation-Graph%EF%BC%89"><span class="toc-text">2.7 计算图（Computation Graph）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-8-%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%AF%BC%E6%95%B0%EF%BC%88Derivatives-with-a-Computation-Graph%EF%BC%89"><span class="toc-text">2.8 计算图导数（Derivatives with a Computation Graph）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-9-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Logistic-Regression-Gradient-Descent%EF%BC%89"><span class="toc-text">2.9 逻辑回归的梯度下降（Logistic Regression Gradient Descent）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-10-m%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Gradient-Descent-on-m-Examples"><span class="toc-text">2.10 m个样本的梯度下降(Gradient Descent on m Examples)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-11-%E5%90%91%E9%87%8F%E5%8C%96-Vectorization"><span class="toc-text">2.11 向量化(Vectorization)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-12-%E5%90%91%E9%87%8F%E5%8C%96%E7%9A%84%E6%9B%B4%E5%A4%9A%E4%BE%8B%E5%AD%90%EF%BC%88More-Examples-of-Vectorization%EF%BC%89"><span class="toc-text">2.12 向量化的更多例子（More Examples of Vectorization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-13-%E5%90%91%E9%87%8F%E5%8C%96logistic%E5%9B%9E%E5%BD%92-Vectorizing-Logistic-Regression"><span class="toc-text">2.13 向量化logistic回归(Vectorizing Logistic Regression)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-14-%E5%90%91%E9%87%8F%E5%8C%96logistic%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%EF%BC%88Vectorizing-Logistic-Regression%E2%80%99s-Gradient%EF%BC%89"><span class="toc-text">2.14 向量化logistic回归的梯度计算（Vectorizing Logistic Regression’s Gradient）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-15-Python%E4%B8%AD%E7%9A%84%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6%EF%BC%88Broadcasting-in-Python%EF%BC%89"><span class="toc-text">2.15 Python中的广播机制（Broadcasting in Python）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-16-%E5%85%B3%E4%BA%8EPython%E4%B8%8Enumpy%E5%90%91%E9%87%8F%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88A-note-on-python-or-numpy-vectors%EF%BC%89"><span class="toc-text">2.16 关于Python与numpy向量的使用（A note on python or numpy vectors）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-17-Jupyter-x2F-iPython-Notebooks%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%EF%BC%88Quick-tour-of-Jupyter-x2F-iPython-Notebooks%EF%BC%89"><span class="toc-text">2.17 Jupyter&#x2F;iPython Notebooks快速入门（Quick tour of Jupyter&#x2F;iPython Notebooks）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-18-logistic%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%88Explanation-of-logistic-regression-cost-function%EF%BC%89"><span class="toc-text">2.18 logistic回归损失函数详解（Explanation of logistic regression cost function）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#C1-3-%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Shallow-neural-networks"><span class="toc-text">C1-3 浅层神经网络(Shallow neural networks)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0%EF%BC%88Neural-Network-Overview%EF%BC%89"><span class="toc-text">3.1 神经网络概述（Neural Network Overview）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA%EF%BC%88Neural-Network-Representation%EF%BC%89"><span class="toc-text">3.2 神经网络的表示（Neural Network Representation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%88Computing-a-Neural-Network%E2%80%99s-output%EF%BC%89"><span class="toc-text">3.3 计算一个神经网络的输出（Computing a Neural Network’s output）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E5%A4%9A%E6%A0%B7%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%88Vectorizing-across-multiple-examples%EF%BC%89"><span class="toc-text">3.4 多样本向量化（Vectorizing across multiple examples）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0%E7%9A%84%E8%A7%A3%E9%87%8A%EF%BC%88Justification-for-vectorized-implementation%EF%BC%89"><span class="toc-text">3.5 向量化实现的解释（Justification for vectorized implementation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-functions%EF%BC%89"><span class="toc-text">3.6 激活函数（Activation functions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F%EF%BC%88why-need-a-nonlinear-activation-function-%EF%BC%89"><span class="toc-text">3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%EF%BC%88Derivatives-of-activation-functions%EF%BC%89"><span class="toc-text">3.8 激活函数的导数（Derivatives of activation functions）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-9-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-descent-for-neural-networks%EF%BC%89"><span class="toc-text">3.9 神经网络的梯度下降（Gradient descent for neural networks）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-10%EF%BC%88%E9%80%89%E4%BF%AE%EF%BC%89%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Backpropagation-intuition%EF%BC%89"><span class="toc-text">3.10（选修）直观理解反向传播（Backpropagation intuition）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-11-%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%88Random-Initialization%EF%BC%89"><span class="toc-text">3.11 随机初始化（Random+Initialization）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#C1-4-%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-Neural-Networks"><span class="toc-text">C1-4 深层神经网络(Deep Neural Networks)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Deep-L-layer-neural-network%EF%BC%89"><span class="toc-text">4.1 深层神经网络（Deep L-layer neural network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%B7%B1%E5%B1%82%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Forward-propagation-in-a-Deep-Network%EF%BC%89"><span class="toc-text">4.2 深层网络中的前向传播（Forward propagation in a Deep Network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E6%A0%B8%E5%AF%B9%E7%9F%A9%E9%98%B5%E7%9A%84%E7%BB%B4%E6%95%B0%EF%BC%88Getting-your-matrix-dimensions-right%EF%BC%89"><span class="toc-text">4.3 核对矩阵的维数（Getting your matrix dimensions right）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%B1%82%E8%A1%A8%E7%A4%BA%EF%BC%9F%EF%BC%88Why-deep-representations-%EF%BC%89"><span class="toc-text">4.4 为什么使用深层表示？（Why deep representations?）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E6%90%AD%E5%BB%BA%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97%EF%BC%88Building-blocks-of-deep-neural-networks%EF%BC%89"><span class="toc-text">4.5 搭建深层神经网络块（Building blocks of deep neural networks）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Forward-and-backward-propagation%EF%BC%89"><span class="toc-text">4.6 前向传播和反向传播（Forward and backward propagation）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7-%E5%8F%82%E6%95%B0VS%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%88Parameters-vs-Hyperparameters%EF%BC%89"><span class="toc-text">4.7 参数VS超参数（Parameters vs Hyperparameters）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-8-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%A4%A7%E8%84%91%E7%9A%84%E5%85%B3%E8%81%94%E6%80%A7%EF%BC%88What-does-this-have-to-do-with-the-brain-%EF%BC%89"><span class="toc-text">4.8 深度学习和大脑的关联性（What does this have to do with the brain?）</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/06/TO-DO-LIST/" title="TO-DO-LIST"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/defaultcover/202206060103934.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="TO-DO-LIST"/></a><div class="content"><a class="title" href="/2022/06/06/TO-DO-LIST/" title="TO-DO-LIST">TO-DO-LIST</a><time datetime="2022-06-05T16:05:23.000Z" title="发表于 2022-06-06 00:05:23">2022-06-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/05/%E8%AE%B0textlint%E7%9A%84%E4%BD%BF%E7%94%A8/" title="记textlint的使用"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206051804098.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="记textlint的使用"/></a><div class="content"><a class="title" href="/2022/06/05/%E8%AE%B0textlint%E7%9A%84%E4%BD%BF%E7%94%A8/" title="记textlint的使用">记textlint的使用</a><time datetime="2022-06-05T10:02:17.000Z" title="发表于 2022-06-05 18:02:17">2022-06-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/30/HIT-%E8%BD%AF%E4%BB%B6%E6%9E%84%E9%80%A0%E5%8D%9A%E5%AE%A25/" title="HIT-软件构造博客5"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206040831943.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="HIT-软件构造博客5"/></a><div class="content"><a class="title" href="/2022/05/30/HIT-%E8%BD%AF%E4%BB%B6%E6%9E%84%E9%80%A0%E5%8D%9A%E5%AE%A25/" title="HIT-软件构造博客5">HIT-软件构造博客5</a><time datetime="2022-05-30T12:34:23.000Z" title="发表于 2022-05-30 20:34:23">2022-05-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/27/HIT-%E8%BD%AF%E4%BB%B6%E6%9E%84%E9%80%A0%E5%8D%9A%E5%AE%A24/" title="HIT-软件构造博客4"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206040831945.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="HIT-软件构造博客4"/></a><div class="content"><a class="title" href="/2022/05/27/HIT-%E8%BD%AF%E4%BB%B6%E6%9E%84%E9%80%A0%E5%8D%9A%E5%AE%A24/" title="HIT-软件构造博客4">HIT-软件构造博客4</a><time datetime="2022-05-27T06:23:45.000Z" title="发表于 2022-05-27 14:23:45">2022-05-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/05/18/HIT-%E8%BD%AF%E4%BB%B6%E6%9E%84%E9%80%A0%E5%8D%9A%E5%AE%A23/" title="HIT-软件构造博客3"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202206040831946.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/pandalandala/imgbed@main/img/202205311419171.jpg'" alt="HIT-软件构造博客3"/></a><div class="content"><a class="title" href="/2022/05/18/HIT-%E8%BD%AF%E4%BB%B6%E6%9E%84%E9%80%A0%E5%8D%9A%E5%AE%A23/" title="HIT-软件构造博客3">HIT-软件构造博客3</a><time datetime="2022-05-18T04:34:42.000Z" title="发表于 2022-05-18 12:34:42">2022-05-18</time></div></div></div></div></div></div></main><footer id="footer" style="background: linear-gradient( 90deg, rgba(247, 149, 51, 0.1) 0, rgba(243, 112, 85, 0.1) 15%, rgba(239, 78, 123, 0.1) 30%, rgba(161, 102, 171, 0.1) 44%, rgba(80, 115, 184, 0.1) 58%, rgba(16, 152, 173, 0.1) 72%, rgba(7, 179, 155, 0.1) 86%, rgba(109, 186, 130, 0.1) 100% );"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By zxr</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>